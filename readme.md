# High-Throughput Sparsity-Aware 2-Way SIMD Neural Engine

## üìñ Introduction

This project presents a **high-performance Fully Connected (FC) Layer Hardware Accelerator** optimized for **Edge AI inference**.

Implemented in **Verilog HDL**, the design features a novel architecture that combines **2-Way SIMD (Single Instruction, Multiple Data) parallelism** with a **Dynamic Sparsity-Aware FSM**. By detecting and skipping zero-valued inputs at runtime (**Zero-Skipping**), the engine significantly reduces computation cycles and energy consumption when processing sparse neural network data (e.g., post-ReLU activations).


---

## ‚ú® Key Features

- **2-Way SIMD Parallelism**  
  Utilizes a *Weight-Stationary* architecture to process **two neurons per clock cycle**, effectively doubling the theoretical throughput.

- **Dynamic Sparsity Awareness**  
  Implements a **Zero-Skipping** mechanism. When zero inputs are detected, the FSM triggers a **Fast Path** (bypassing multiplication and accumulation), reducing latency by approximately **40%** for sparse datasets.

- **Industrial Quantization Scheme**  
  - Inputs / Weights: **8-bit Signed Integer (INT8)**  
  - Accumulator: **24-bit Signed Integer (INT24)** to prevent overflow  
  - Activation: **Hardware-based ReLU with Saturation logic**

- **Robust Interface**  
  Features a decoupled **Request / Acknowledge handshake protocol** (`input_req` / `out_valid`) for seamless integration with memory subsystems.

---

## üìÇ File Structure

| File Name | Type | Description |
|----------|------|-------------|
| `fc_layer.v` | Verilog | Core RTL design. Contains the SIMD datapath, Sparsity-Aware FSM, and ALU logic. |
| `tb_fc_layer.v` | Verilog | Testbench. Reads HEX files, drives the DUT, and performs bit-accurate verification against the Golden Model. |
| `model_gen.py` | Python | Golden model script. Generates synthetic test vectors with controlled sparsity (injected zeros) and computes expected outputs. |
| `*.hex` | Data | Simulation data generated by the Python script: `inputs.hex`, `weights.hex`, `biases.hex`, `golden_outputs.hex`. |
| `VLSI.pdf` | Report | Final project report with architecture diagrams, timing analysis, and waveform interpretation. |

---

## üöÄ Quick Start

### 1. Prerequisites

Ensure the following tools are installed:

- **Python 3** (for data generation)
- **Icarus Verilog** (or Vivado / ModelSim)
- **GTKWave** (for waveform visualization)

---

### 2. Generate Test Data

Run the Python golden model to generate fresh test vectors, including sparse zero-injections:

```bash
python3 model_gen.py
```

This updates the `.hex` files in the working directory.

---

### 3. Run Simulation

Compile and simulate using Icarus Verilog:

```bash
# Compile RTL and testbench
iverilog -o sim_output fc_layer.v tb_fc_layer.v

# Execute simulation
vvp sim_output
```

---

### 4. Verify Results

Expected console output:

```text
Starting Simulation for 5 Test Cases
-----------------------------------------
TEST PASSED! All outputs match Golden Model.
-----------------------------------------
```

To visualize **Zero-Skipping behavior** (Fast Path vs. Full Path):

```bash
gtkwave wave.vcd
```

---

## üìä Performance & Logic


- **Fast Path (Zero Input):** 3 clock cycles  
  Arithmetic logic is bypassed when inputs are zero.

- **Full Path (Non-Zero Input):** 5 clock cycles  
  Full Multiply-Accumulate pipeline is executed.

**Result:** Variable-latency execution that significantly improves efficiency on sparse workloads commonly found in CNNs and DNNs.

---

## ‚öôÔ∏è Configuration

The design is fully parameterized for scalability (`fc_layer_final.v`):

```verilog
parameter DATA_WIDTH  = 8;   // Input / output bit-width
parameter ACC_WIDTH   = 24;  // Internal accumulator width
parameter NUM_NEURONS = 10;  // Number of output neurons
parameter NUM_INPUTS  = 4;   // Input feature dimension
```


---

## üìÑ License

This project is licensed under the **MIT License**.

